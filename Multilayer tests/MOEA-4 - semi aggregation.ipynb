{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2layer_mu0_1_v2 = ['mLFR3/2layer_mu0.1_2_Layer1_Network.txt.txt', 'mLFR3/2layer_mu0.1_2_Layer2_Network.txt.txt']\n",
    "_2layer_mu0_1_multi = ['mLFR3/2lazer_mu0.1_multi_Layer1_Network.txt.txt', 'mLFR3/2lazer_mu0.1_multi_Layer2_Network.txt.txt']\n",
    "_3layer_mu0_1_multi = ['mLFR3/3layer_mu0.1_multi_Layer1_Network.txt.txt', 'mLFR3/3layer_mu0.1_multi_Layer2_Network.txt.txt', 'mLFR3/3layer_mu0.1_multi_Layer3_Network.txt.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2layer_mu0_0 = ['mLFR2/2layer_mu0_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0_Layer2_Network.txt.txt']\n",
    "_2layer_mu0_1 = ['mLFR2/2layer_mu0.1_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0.1_Layer2_Network.txt.txt']\n",
    "_2layer_mu0_2 = ['mLFR2/2layer_mu0.2_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0.2_Layer2_Network.txt.txt']\n",
    "_2layer_mu0_3 = ['mLFR2/2layer_mu0.3_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0.3_Layer2_Network.txt.txt']\n",
    "_2layer_mu0_4 = ['mLFR2/2layer_mu0.4_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0.4_Layer2_Network.txt.txt']\n",
    "_2layer_mu0_5 = ['mLFR2/2layer_mu0.5_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0.5_Layer2_Network.txt.txt']\n",
    "\n",
    "_3layer_mu0_0 = ['mLFR2/3layer_mu0_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0_Layer3_Network.txt.txt']\n",
    "_3layer_mu0_1 = ['mLFR2/3layer_mu0.1_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0.1_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0.1_Layer3_Network.txt.txt']\n",
    "_3layer_mu0_2 = ['mLFR2/3layer_mu0.2_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0.2_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0.2_Layer3_Network.txt.txt']\n",
    "_3layer_mu0_3 = ['mLFR2/3layer_mu0.3_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0.3_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0.3_Layer3_Network.txt.txt']\n",
    "_3layer_mu0_4 = ['mLFR2/3layer_mu0.4_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0.4_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0.4_Layer3_Network.txt.txt']\n",
    "_3layer_mu0_5 = ['mLFR2/3layer_mu0.5_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0.5_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0.5_Layer3_Network.txt.txt']\n",
    "\n",
    "_4layer_mu0_0 = ['mLFR2/4layer_mu0_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0_Layer4_Network.txt.txt']\n",
    "_4layer_mu0_1 = ['mLFR2/4layer_mu0.1_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0.1_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0.1_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0.1_Layer4_Network.txt.txt']\n",
    "_4layer_mu0_2 = ['mLFR2/4layer_mu0.2_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0.2_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0.2_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0.2_Layer4_Network.txt.txt']\n",
    "_4layer_mu0_3 = ['mLFR2/4layer_mu0.3_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0.3_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0.3_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0.3_Layer4_Network.txt.txt']\n",
    "_4layer_mu0_4 = ['mLFR2/4layer_mu0.4_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0.4_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0.4_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0.4_Layer4_Network.txt.txt']\n",
    "_4layer_mu0_5 = ['mLFR2/4layer_mu0.5_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0.5_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0.5_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0.5_Layer4_Network.txt.txt']\n",
    "\n",
    "testData = [_2layer_mu0_0, _2layer_mu0_1, _2layer_mu0_2, _2layer_mu0_3, _2layer_mu0_4, _2layer_mu0_5, _3layer_mu0_0, _3layer_mu0_1, _3layer_mu0_2, _3layer_mu0_3, _3layer_mu0_4, _3layer_mu0_5, _4layer_mu0_0, _4layer_mu0_1, _4layer_mu0_2, _4layer_mu0_3, _4layer_mu0_4, _4layer_mu0_5]\n",
    "\n",
    "testData2 = [_2layer_mu0_0, _2layer_mu0_1, _2layer_mu0_2, _2layer_mu0_3, _2layer_mu0_4, _2layer_mu0_5]\n",
    "testData3 = [_3layer_mu0_0, _3layer_mu0_1, _3layer_mu0_2, _3layer_mu0_3, _3layer_mu0_4, _3layer_mu0_5]\n",
    "testData4 = [_4layer_mu0_0, _4layer_mu0_1, _4layer_mu0_2, _4layer_mu0_3, _4layer_mu0_4, _4layer_mu0_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aarhus = ['aarhus_1.txt', 'aarhus_2.txt', 'aarhus_3.txt', 'aarhus_4.txt', 'aarhus_5.txt']\n",
    "euair = ['euair_1.txt', 'euair_10.txt', 'euair_11.txt', 'euair_12.txt', 'euair_13.txt', 'euair_14.txt', 'euair_15.txt', 'euair_16.txt', 'euair_17.txt', 'euair_18.txt', 'euair_19.txt', 'euair_2.txt', 'euair_20.txt', 'euair_21.txt', 'euair_22.txt', 'euair_23.txt', 'euair_24.txt', 'euair_25.txt', 'euair_26.txt', 'euair_27.txt', 'euair_28.txt', 'euair_29.txt', 'euair_3.txt', 'euair_30.txt', 'euair_31.txt', 'euair_32.txt', 'euair_33.txt', 'euair_34.txt', 'euair_35.txt', 'euair_36.txt', 'euair_37.txt', 'euair_4.txt', 'euair_5.txt', 'euair_6.txt', 'euair_7.txt', 'euair_8.txt', 'euair_9.txt']\n",
    "krackhardt = ['krackhardt_1.txt', 'krackhardt_2.txt', 'krackhardt_3.txt']\n",
    "padgett = ['padgett_1.txt', 'padgett_2.txt']\n",
    "_4layers = ['output_4layer_1.txt', 'output_4layer_2.txt', 'output_4layer_3.txt', 'output_4layer_4.txt']\n",
    "_3layers = ['output_3layer_1.txt', 'output_3layer_2.txt', 'output_3layer_3.txt']\n",
    "_mlfr_2layer = ['mLFR/2layer_mlfr_Layer1_.txt', 'mLFR/2layer_mlfr_Layer2_.txt']\n",
    "_mlfr_3layer = ['mLFR/3layer_mlfr_Layer1_.txt', 'mLFR/3layer_mlfr_Layer2_.txt', 'mLFR/3layer_mlfr_Layer3_.txt']\n",
    "_mlfr_4layer = ['mLFR/4layer_mlfr_Layer1_.txt', 'mLFR/4layer_mlfr_Layer2_.txt', 'mLFR/4layer_mlfr_Layer3_.txt', 'mLFR/4layer_mlfr_Layer4_.txt']\n",
    "_mlfr_4layer_overlap = ['mLFR/4layer_mlfr_overlap_Layer1_.txt', 'mLFR/4layer_mlfr_overlap_Layer2_.txt', 'mLFR/4layer_mlfr_overlap_Layer3_.txt', 'mLFR/4layer_mlfr_overlap_Layer4_.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.06 ms\n"
     ]
    }
   ],
   "source": [
    "_2layer_mu0_1_multi = ['mLFR4/2layer_mu0.1_multi_Layer1_Network.txt.txt', 'mLFR4/2layer_mu0.1_multi_Layer2_Network.txt.txt']\n",
    "_2layer_mu0_2_multi = ['mLFR4/2layer_mu0.2_multi_Layer1_Network.txt.txt', 'mLFR4/2layer_mu0.2_multi_Layer2_Network.txt.txt']\n",
    "_2layer_mu0_3_multi = ['mLFR4/2layer_mu0.3_multi_Layer1_Network.txt.txt', 'mLFR4/2layer_mu0.3_multi_Layer2_Network.txt.txt']\n",
    "_3layer_mu0_1_multi = ['mLFR4/3layer_mu0.1_multi_Layer1_Network.txt.txt', 'mLFR4/3layer_mu0.1_multi_Layer2_Network.txt.txt', 'mLFR4/3layer_mu0.1_multi_Layer3_Network.txt.txt']\n",
    "_3layer_mu0_2_multi = ['mLFR4/3layer_mu0.2_multi_Layer1_Network.txt.txt', 'mLFR4/3layer_mu0.2_multi_Layer2_Network.txt.txt', 'mLFR4/3layer_mu0.2_multi_Layer3_Network.txt.txt']\n",
    "_3layer_mu0_3_multi = ['mLFR4/3layer_mu0.3_multi_Layer1_Network.txt.txt', 'mLFR4/3layer_mu0.3_multi_Layer2_Network.txt.txt', 'mLFR4/3layer_mu0.3_multi_Layer3_Network.txt.txt']\n",
    "_4layer_mu0_1_multi = ['mLFR4/4layer_mu0.1_multi_Layer1_Network.txt.txt', 'mLFR4/4layer_mu0.1_multi_Layer2_Network.txt.txt', 'mLFR4/4layer_mu0.1_multi_Layer3_Network.txt.txt', 'mLFR4/4layer_mu0.1_multi_Layer4_Network.txt.txt']\n",
    "_4layer_mu0_2_multi = ['mLFR4/4layer_mu0.2_multi_Layer1_Network.txt.txt', 'mLFR4/4layer_mu0.2_multi_Layer2_Network.txt.txt', 'mLFR4/4layer_mu0.2_multi_Layer3_Network.txt.txt', 'mLFR4/4layer_mu0.2_multi_Layer4_Network.txt.txt']\n",
    "_4layer_mu0_3_multi = ['mLFR4/4layer_mu0.3_multi_Layer1_Network.txt.txt', 'mLFR4/4layer_mu0.3_multi_Layer2_Network.txt.txt', 'mLFR4/4layer_mu0.3_multi_Layer3_Network.txt.txt', 'mLFR4/4layer_mu0.3_multi_Layer4_Network.txt.txt']\n",
    "\n",
    "testData = [_2layer_mu0_1_multi, _2layer_mu0_2_multi, _2layer_mu0_3_multi, _3layer_mu0_1_multi, _3layer_mu0_2_multi, _3layer_mu0_3_multi, _4layer_mu0_1_multi, _4layer_mu0_2_multi, _4layer_mu0_3_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import pygmo as pg\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import community as comm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime, time, os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.63 ms\n"
     ]
    }
   ],
   "source": [
    "class community_function:\n",
    "  def __init__(self, fileNames, alpha=None):\n",
    "    self.fileNames = fileNames\n",
    "    self.G = []\n",
    "    for fileName in self.fileNames:\n",
    "      g = self.getNeighbourMatrixFromFile(fileName)\n",
    "      self.G.append(g)\n",
    "    self.dim = len(list(self.G[0].nodes))\n",
    "    self.layers = len(fileNames)\n",
    "    \n",
    "    if alpha is None:\n",
    "      alpha = np.full(self.layers, 0.5)\n",
    "    self.H = []\n",
    "    for i in range(self.layers):\n",
    "      if alpha is None:\n",
    "        self.alpha = np.full(self.layers, 0.5)\n",
    "      else:\n",
    "        self.alpha = np.full(self.layers, alpha)\n",
    "      H = self.mergeLayers(i, self.alpha)\n",
    "      self.H.append(H)\n",
    "      \n",
    "  def fitness(self, x):\n",
    "    partition = {}\n",
    "    for val, ind in zip(np.nditer(x), np.ndindex(x.shape[0])):\n",
    "      partition[ind[0]] = int(val)\n",
    "      \n",
    "    modularity = 0\n",
    "    for g in self.H:\n",
    "      modularity += comm.modularity(partition, g)\n",
    "    if modularity < 0:\n",
    "      modularity = 0\n",
    "    return [-modularity]\n",
    "\n",
    "  def get_bounds(self):\n",
    "    return ([1] * self.dim, [self.dim] * self.dim)\n",
    "  \n",
    "  def get_nix(self):\n",
    "    # Number of integer dimensions (all of them)\n",
    "    print(self.dim)\n",
    "    return self.dim\n",
    "  \n",
    "  def get_name(self):\n",
    "    ret = \"Best partition for multiplex graph\\n\"\n",
    "    for fileName in self.fileNames:\n",
    "      ret +=\"File: \" + fileName + \"\\n\"\n",
    "    ret += \"Dimension: \" + str(self.dim)\n",
    "    ret += \"\\n\\n\"\n",
    "    return ret\n",
    "  \n",
    "  def getNeighbourMatrixFromFile(self, fileName):\n",
    "    A = pd.read_csv(fileName, sep=\" \", header=None)\n",
    "    G = nx.from_numpy_matrix(np.array(A))\n",
    "    return G\n",
    "  \n",
    "  def drawPartition(self, graphIndex=0, partition=None, fun=nx.spring_layout, mergedDraw=False, fileNameEndsWith=\"_moeaGenerated.png\"):\n",
    "    if partition is None:\n",
    "      raise ValueError(\"No partition passed as parameter!!\")\n",
    "    if type(partition) is list:\n",
    "      partition = {key:val for key, val in enumerate(partition)}\n",
    "    if type(partition) is not dict:\n",
    "      raise ValueError(\"Invalid partititon as input: \" + str(type(partition)))\n",
    "\n",
    "    plt.clf()\n",
    "      \n",
    "    colors = self.getColors()\n",
    "    if not mergedDraw:\n",
    "      G = self.G[graphIndex]\n",
    "    else:\n",
    "      G = self.H[graphIndex]\n",
    "      for i in range(self.dim):\n",
    "        G.add_node(i)\n",
    "    pos = fun(G)\n",
    "    count = 0\n",
    "    \n",
    "    for com in set(partition.values()):\n",
    "      toDraw = True\n",
    "      list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "      labels = dict()\n",
    "      for node in list_nodes:\n",
    "        labels[node] = node+1\n",
    "      node_colors = np.zeros((len(list_nodes), 3))\n",
    "      if mergedDraw:\n",
    "        try:\n",
    "          node_colors[:] = colors[count]\n",
    "          count += 1\n",
    "        except:\n",
    "          node_colors[:] = [1, 1, 1]\n",
    "      else:\n",
    "        if len(list_nodes) <= 3:\n",
    "          node_colors[:] = [1, 1, 1]\n",
    "          if fun != nx.circular_layout:\n",
    "            toDraw = False\n",
    "        else:\n",
    "          try:\n",
    "            node_colors[:] = colors[count]\n",
    "            count += 1\n",
    "          except:\n",
    "            node_colors[:] = [1, 1, 1]\n",
    "      if toDraw:\n",
    "        nx.draw_networkx_nodes(G, pos, list_nodes, node_size=20, node_color=node_colors)\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=3)\n",
    "    nx.draw_networkx_edges(G, pos, width=0.2)\n",
    "    if mergedDraw:\n",
    "      labels = nx.get_edge_attributes(G,'weight')\n",
    "      nx.draw_networkx_edge_labels(G,pos,edge_labels=labels, font_size=3)\n",
    "    \n",
    "    if mergedDraw:\n",
    "      fileNameEndsWith = \"merged_view_\" + fileNameEndsWith\n",
    "    outfile=self.fileNames[graphIndex] + fileNameEndsWith\n",
    "    plt.savefig(outfile, dpi=300)\n",
    "    \n",
    "  def getColors(self):\n",
    "    return np.array([\n",
    "            [230, 25, 75],\n",
    "            [60, 180, 75],\n",
    "            [255, 225, 25],\n",
    "            [0, 130, 200],\n",
    "            [245, 130, 48],\n",
    "            [145, 30, 180],\n",
    "            [70, 240, 240],\n",
    "            [240, 50, 230],\n",
    "            [210, 245, 60],\n",
    "            [250, 190, 190],\n",
    "            [0, 128, 128],\n",
    "            [230, 190, 255],\n",
    "            [170, 110, 40],\n",
    "            [255, 250, 200],\n",
    "            [128, 0, 0],\n",
    "            [170, 255, 195],\n",
    "            [128, 128, 0],\n",
    "            [255, 215, 180],\n",
    "            [0, 0, 128],\n",
    "            [128, 128, 128],\n",
    "            [0, 0, 0]\n",
    "          ])/255\n",
    "  \n",
    "\n",
    "  def mergeLayers(self, mainLayer, alpha):\n",
    "    '''\n",
    "      Merge the layers into one graph but use alpha scaling for layers. The main layers scale is always 1\n",
    "      \n",
    "      mainLayer : integer\n",
    "                index of main layer\n",
    "      alpha : list\n",
    "    '''\n",
    "    alpha[mainLayer] = 1 #take the mainLayer's weights without scaling\n",
    "    H = nx.Graph()\n",
    "    for i in range(len(self.G)):\n",
    "      g = self.G[i]\n",
    "      for e in g.edges().data('weight'):\n",
    "        (u, v, weight) = e\n",
    "        if (u,v) not in H.edges():\n",
    "          H.add_edge(u,v, weight=weight*alpha[i])\n",
    "        else:\n",
    "          H[u][v]['weight'] = H[u][v]['weight'] + weight*alpha[i]\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.0976872640105713,\n",
       " -2.160827668833295,\n",
       " -2.1419457750663478,\n",
       " -2.1609928705186716,\n",
       " -2.1195652844868493,\n",
       " -2.19368593871474,\n",
       " -2.1930678528718737,\n",
       " -2.155757142171788,\n",
       " -2.1287281571627332,\n",
       " -2.1797753546858463,\n",
       " -2.145726922234128,\n",
       " -2.1495171262346764,\n",
       " -2.177426926506766,\n",
       " -2.1655599573835644,\n",
       " -2.1353730205695443,\n",
       " -2.1544801777973137,\n",
       " -2.1917161585007996,\n",
       " -2.1429672356934653,\n",
       " -2.1897193882139523,\n",
       " -2.1932154593382]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.75 ms\n"
     ]
    }
   ],
   "source": [
    "for i in f:\n",
    "  ff.append(i[0])\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 51, 38)\n",
      "0.1\n",
      "128\n",
      "(18, 51, 38)\n",
      "0.2\n",
      "128\n",
      "(18, 51, 39)\n",
      "0.3\n",
      "128\n",
      "(18, 51, 40)\n",
      "0.4\n",
      "128\n",
      "(18, 51, 41)\n",
      "0.5\n",
      "128\n",
      "(18, 51, 41)\n",
      "0.6\n",
      "128\n",
      "(19, 42, 55)\n",
      "\n",
      "(19, 42, 55)\n",
      "\n",
      "(19, 42, 57)\n",
      "\n",
      "(19, 44, 7)\n",
      "\n",
      "(19, 44, 7)\n",
      "\n",
      "(19, 44, 7)\n",
      "\n",
      "(19, 44, 7)\n",
      "0.7\n",
      "128\n",
      "(19, 44, 8)\n",
      "0.8\n",
      "128\n",
      "(19, 44, 8)\n",
      "0.9\n",
      "128\n",
      "(19, 44, 9)\n",
      "1.0\n",
      "128\n",
      "(19, 44, 10)\n",
      "\n",
      "(20, 34, 8)\n",
      "\n",
      "(20, 34, 17)\n",
      "\n",
      "(20, 36, 17)\n",
      "\n",
      "time: 1h 44min 47s\n"
     ]
    }
   ],
   "source": [
    "f = []\n",
    "alpha = 0\n",
    "i = -1\n",
    "land = []\n",
    "\n",
    "while alpha < 1:\n",
    "  alpha += 0.1\n",
    "  alpha = np.around(alpha, 2)\n",
    "  i += 1\n",
    "  now = datetime.datetime.now()\n",
    "  time_now = (now.hour, now.minute, now.second)\n",
    "  print(time_now)\n",
    "  print(alpha)\n",
    "  \n",
    "  problem = pg.problem(community_function(_4layer_mu0_1_multi, alpha=alpha))\n",
    "  algo = pg.algorithm(pg.sga(gen=6000))\n",
    "  \n",
    "  archi = pg.archipelago(n=1, algo=algo, prob=problem, pop_size=50)\n",
    "  archi.evolve()\n",
    "  land.append({\n",
    "    'archi': archi,\n",
    "    'alpha': alpha\n",
    "  })\n",
    "  \n",
    "  if (i >= 5):\n",
    "    i = 0\n",
    "    for l in land:\n",
    "      archi = l['archi']\n",
    "      a = l['alpha']\n",
    "      archi.wait()\n",
    "      champion_x = archi.get_champions_x()[0]\n",
    "      champion_f = archi.get_champions_f()[0]\n",
    "      f.append(champion_f)\n",
    "      df = pd.DataFrame()\n",
    "      df[0] = list(range(0, len(champion_x)))\n",
    "      df[1] = champion_x.astype(int)\n",
    "      df.to_csv('semiAggAlphaL4_2_' + str(a) + '.txt', sep=' ', header=False, index=False)\n",
    "      land = []\n",
    "      \n",
    "      now = datetime.datetime.now()\n",
    "      time_now = (now.hour, now.minute, now.second)\n",
    "      print(time_now)\n",
    "      print(\"\")\n",
    "  \n",
    "for l in land:\n",
    "  now = datetime.datetime.now()\n",
    "  time_now = (now.hour, now.minute, now.second)\n",
    "  print(time_now)\n",
    "  print(\"\")\n",
    "  archi = l['archi']\n",
    "  a = l['alpha']\n",
    "  archi.wait()\n",
    "  champion_x = archi.get_champions_x()[0]\n",
    "  champion_f = archi.get_champions_f()[0]\n",
    "  f.append(champion_f)\n",
    "  df = pd.DataFrame()\n",
    "  df[0] = list(range(0, len(champion_x)))\n",
    "  df[1] = champion_x.astype(int)\n",
    "  df.to_csv('semiAggAlphaL4_2_' + str(a) + '.txt', sep=' ', header=False, index=False)\n",
    "  land = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 327 µs\n"
     ]
    }
   ],
   "source": [
    "# problem = pg.problem(community_function(_3layer_mu0_1_multi))\n",
    "# algo = pg.algorithm(pg.sga(gen=10000))\n",
    "# algo.set_verbosity(200)\n",
    "# archi = pg.archipelago(n=6, algo=algo, prob=problem, pop_size=50)\n",
    "# archi.evolve()\n",
    "# archi.wait()\n",
    "# champions = archi.get_champions_f()\n",
    "# archi.get_champions_x()\n",
    "# globalMin = min(champions)\n",
    "# globalMin\n",
    "# championIndex = champions.index(globalMin)\n",
    "# champion_x = archi.get_champions_x()[championIndex]\n",
    "# df = pd.DataFrame()\n",
    "# df[0] = list(range(0, len(champion_x)))\n",
    "# df[1] = champion_x.astype(int)\n",
    "# df.to_csv('test6.txt', sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "2layermu0_all_community_merged_run1.txt\n",
      "[0.80279394]\n",
      "(14, 50, 4)\n",
      "\n",
      "128\n",
      "2layermu0.1_all_community_merged_run1.txt\n",
      "[1.27527839]\n",
      "(15, 1, 23)\n",
      "\n",
      "128\n",
      "2layermu0.2_all_community_merged_run1.txt\n",
      "[1.25523601]\n",
      "(15, 12, 34)\n",
      "\n",
      "128\n",
      "2layermu0.3_all_community_merged_run1.txt\n",
      "[1.4262927]\n",
      "(15, 23, 34)\n",
      "\n",
      "128\n",
      "2layermu0.4_all_community_merged_run1.txt\n",
      "[1.36215107]\n",
      "(15, 34, 47)\n",
      "\n",
      "128\n",
      "2layermu0.5_all_community_merged_run1.txt\n",
      "[1.32642302]\n",
      "(15, 47, 21)\n",
      "\n",
      "time: 1h 8min 16s\n"
     ]
    }
   ],
   "source": [
    "for data in testData:\n",
    "  outFileName = ''.join(data[0].split('_')[0:2]).split('/')[1] + '_all_community_semi_agg_run1.txt'\n",
    "  os.write(1, outFileName.encode())\n",
    "  \n",
    "  problem = pg.problem(community_function(data))\n",
    "  algo = pg.algorithm(pg.sga(gen=10000))\n",
    "  algo.set_verbosity(100)\n",
    "  archi = pg.archipelago(n=3, algo=algo, prob=problem, pop_size=50)\n",
    "  archi.evolve()\n",
    "  archi.wait()\n",
    "  \n",
    "  champions = archi.get_champions_f()\n",
    "  archi.get_champions_x()\n",
    "  globalMin = min(champions)\n",
    "  championIndex = champions.index(globalMin)\n",
    "  champion_x = archi.get_champions_x()[championIndex]\n",
    "    \n",
    "  df = pd.DataFrame()\n",
    "  df[0] = list(range(0, len(champion_x)))\n",
    "  df[1] = champion_x.astype(int)\n",
    "  \n",
    "  df.to_csv(outFileName, sep=' ', header=False, index=False)\n",
    "  \n",
    "  print(outFileName)\n",
    "  print(globalMin)\n",
    "  \n",
    "  now = datetime.datetime.now()\n",
    "  time_now = (now.hour, now.minute, now.second)\n",
    "  print(time_now)\n",
    "  print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
