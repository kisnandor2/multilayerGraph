{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aarhus = ['aarhus_1.txt', 'aarhus_2.txt', 'aarhus_3.txt', 'aarhus_4.txt', 'aarhus_5.txt']\n",
    "# euair = ['euair_1.txt', 'euair_10.txt', 'euair_11.txt', 'euair_12.txt', 'euair_13.txt', 'euair_14.txt', 'euair_15.txt', 'euair_16.txt', 'euair_17.txt', 'euair_18.txt', 'euair_19.txt', 'euair_2.txt', 'euair_20.txt', 'euair_21.txt', 'euair_22.txt', 'euair_23.txt', 'euair_24.txt', 'euair_25.txt', 'euair_26.txt', 'euair_27.txt', 'euair_28.txt', 'euair_29.txt', 'euair_3.txt', 'euair_30.txt', 'euair_31.txt', 'euair_32.txt', 'euair_33.txt', 'euair_34.txt', 'euair_35.txt', 'euair_36.txt', 'euair_37.txt', 'euair_4.txt', 'euair_5.txt', 'euair_6.txt', 'euair_7.txt', 'euair_8.txt', 'euair_9.txt']\n",
    "# krackhardt = ['krackhardt_1.txt', 'krackhardt_2.txt', 'krackhardt_3.txt']\n",
    "# padgett = ['padgett_1.txt', 'padgett_2.txt']\n",
    "# _4layers = ['output_4layer_1.txt', 'output_4layer_2.txt', 'output_4layer_3.txt', 'output_4layer_4.txt']\n",
    "# _3layers = ['output_3layer_1.txt', 'output_3layer_2.txt', 'output_3layer_3.txt']\n",
    "# _mlfr_2layer = ['mLFR/2layer_mlfr_Layer1_.txt', 'mLFR/2layer_mlfr_Layer2_.txt']\n",
    "# _mlfr_3layer = ['mLFR/3layer_mlfr_Layer1_.txt', 'mLFR/3layer_mlfr_Layer2_.txt', 'mLFR/3layer_mlfr_Layer3_.txt']\n",
    "# _mlfr_4layer = ['mLFR/4layer_mlfr_Layer1_.txt', 'mLFR/4layer_mlfr_Layer2_.txt', 'mLFR/4layer_mlfr_Layer3_.txt', 'mLFR/4layer_mlfr_Layer4_.txt']\n",
    "# _mlfr_4layer_overlap = ['mLFR/4layer_mlfr_overlap_Layer1_.txt', 'mLFR/4layer_mlfr_overlap_Layer2_.txt', 'mLFR/4layer_mlfr_overlap_Layer3_.txt', 'mLFR/4layer_mlfr_overlap_Layer4_.txt']\n",
    "\n",
    "# _2layer_mu0_0 = ['mLFR2/2layer_mu0_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0_Layer2_Network.txt.txt']\n",
    "# _2layer_mu0_1 = ['mLFR2/2layer_mu0.1_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0.1_Layer2_Network.txt.txt']\n",
    "# _2layer_mu0_2 = ['mLFR2/2layer_mu0.2_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0.2_Layer2_Network.txt.txt']\n",
    "# _2layer_mu0_3 = ['mLFR2/2layer_mu0.3_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0.3_Layer2_Network.txt.txt']\n",
    "# _2layer_mu0_4 = ['mLFR2/2layer_mu0.4_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0.4_Layer2_Network.txt.txt']\n",
    "# _2layer_mu0_5 = ['mLFR2/2layer_mu0.5_Layer1_Network.txt.txt', 'mLFR2/2layer_mu0.5_Layer2_Network.txt.txt']\n",
    "\n",
    "# _3layer_mu0_0 = ['mLFR2/3layer_mu0_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0_Layer3_Network.txt.txt']\n",
    "# _3layer_mu0_1 = ['mLFR2/3layer_mu0.1_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0.1_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0.1_Layer3_Network.txt.txt']\n",
    "# _3layer_mu0_2 = ['mLFR2/3layer_mu0.2_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0.2_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0.2_Layer3_Network.txt.txt']\n",
    "# _3layer_mu0_3 = ['mLFR2/3layer_mu0.3_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0.3_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0.3_Layer3_Network.txt.txt']\n",
    "# _3layer_mu0_4 = ['mLFR2/3layer_mu0.4_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0.4_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0.4_Layer3_Network.txt.txt']\n",
    "# _3layer_mu0_5 = ['mLFR2/3layer_mu0.5_Layer1_Network.txt.txt', 'mLFR2/3layer_mu0.5_Layer2_Network.txt.txt', 'mLFR2/3layer_mu0.5_Layer3_Network.txt.txt']\n",
    "\n",
    "# _4layer_mu0_0 = ['mLFR2/4layer_mu0_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0_Layer4_Network.txt.txt']\n",
    "# _4layer_mu0_1 = ['mLFR2/4layer_mu0.1_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0.1_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0.1_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0.1_Layer4_Network.txt.txt']\n",
    "# _4layer_mu0_2 = ['mLFR2/4layer_mu0.2_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0.2_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0.2_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0.2_Layer4_Network.txt.txt']\n",
    "# _4layer_mu0_3 = ['mLFR2/4layer_mu0.3_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0.3_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0.3_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0.3_Layer4_Network.txt.txt']\n",
    "# _4layer_mu0_4 = ['mLFR2/4layer_mu0.4_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0.4_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0.4_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0.4_Layer4_Network.txt.txt']\n",
    "# _4layer_mu0_5 = ['mLFR2/4layer_mu0.5_Layer1_Network.txt.txt', 'mLFR2/4layer_mu0.5_Layer2_Network.txt.txt', 'mLFR2/4layer_mu0.5_Layer3_Network.txt.txt', 'mLFR2/4layer_mu0.5_Layer4_Network.txt.txt']\n",
    "\n",
    "# testData = [_2layer_mu0_0, _2layer_mu0_1, _2layer_mu0_2, _2layer_mu0_3, _2layer_mu0_4, _2layer_mu0_5, _3layer_mu0_0, _3layer_mu0_1, _3layer_mu0_2, _3layer_mu0_3, _3layer_mu0_4, _3layer_mu0_5, _4layer_mu0_0, _4layer_mu0_1, _4layer_mu0_2, _4layer_mu0_3, _4layer_mu0_4, _4layer_mu0_5]\n",
    "# testData2 = [_4layer_mu0_0, _4layer_mu0_1, _4layer_mu0_2, _4layer_mu0_3, _4layer_mu0_4, _4layer_mu0_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2layer_mu0_1_multi = ['mLFR4/2layer_mu0.1_multi_Layer1_Network.txt.txt', 'mLFR4/2layer_mu0.1_multi_Layer2_Network.txt.txt']\n",
    "_2layer_mu0_2_multi = ['mLFR4/2layer_mu0.2_multi_Layer1_Network.txt.txt', 'mLFR4/2layer_mu0.2_multi_Layer2_Network.txt.txt']\n",
    "_2layer_mu0_3_multi = ['mLFR4/2layer_mu0.3_multi_Layer1_Network.txt.txt', 'mLFR4/2layer_mu0.3_multi_Layer2_Network.txt.txt']\n",
    "_3layer_mu0_1_multi = ['mLFR4/3layer_mu0.1_multi_Layer1_Network.txt.txt', 'mLFR4/3layer_mu0.1_multi_Layer2_Network.txt.txt', 'mLFR4/3layer_mu0.1_multi_Layer3_Network.txt.txt']\n",
    "_3layer_mu0_2_multi = ['mLFR4/3layer_mu0.2_multi_Layer1_Network.txt.txt', 'mLFR4/3layer_mu0.2_multi_Layer2_Network.txt.txt', 'mLFR4/3layer_mu0.2_multi_Layer3_Network.txt.txt']\n",
    "_3layer_mu0_3_multi = ['mLFR4/3layer_mu0.3_multi_Layer1_Network.txt.txt', 'mLFR4/3layer_mu0.3_multi_Layer2_Network.txt.txt', 'mLFR4/3layer_mu0.3_multi_Layer3_Network.txt.txt']\n",
    "_4layer_mu0_1_multi = ['mLFR4/4layer_mu0.1_multi_Layer1_Network.txt.txt', 'mLFR4/4layer_mu0.1_multi_Layer2_Network.txt.txt', 'mLFR4/4layer_mu0.1_multi_Layer3_Network.txt.txt', 'mLFR4/4layer_mu0.1_multi_Layer4_Network.txt.txt']\n",
    "_4layer_mu0_2_multi = ['mLFR4/4layer_mu0.2_multi_Layer1_Network.txt.txt', 'mLFR4/4layer_mu0.2_multi_Layer2_Network.txt.txt', 'mLFR4/4layer_mu0.2_multi_Layer3_Network.txt.txt', 'mLFR4/4layer_mu0.2_multi_Layer4_Network.txt.txt']\n",
    "_4layer_mu0_3_multi = ['mLFR4/4layer_mu0.3_multi_Layer1_Network.txt.txt', 'mLFR4/4layer_mu0.3_multi_Layer2_Network.txt.txt', 'mLFR4/4layer_mu0.3_multi_Layer3_Network.txt.txt', 'mLFR4/4layer_mu0.3_multi_Layer4_Network.txt.txt']\n",
    "\n",
    "testData = [_2layer_mu0_1_multi, _2layer_mu0_2_multi, _2layer_mu0_3_multi, _3layer_mu0_1_multi, _3layer_mu0_2_multi, _3layer_mu0_3_multi, _4layer_mu0_1_multi, _4layer_mu0_2_multi, _4layer_mu0_3_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Audio\n",
    "# sound_file = './beep-01a.wav'\n",
    "# Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import pygmo as pg\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import community as comm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_3layer_mu0_1_multi = ['mLFR3/3layer_mu0.1_multi_Layer1_Network.txt.txt', 'mLFR3/3layer_mu0.1_multi_Layer2_Network.txt.txt', 'mLFR3/3layer_mu0.1_multi_Layer3_Network.txt.txt']\n",
    "\n",
    "import datetime, time, os\n",
    "\n",
    "%%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.5 ms\n"
     ]
    }
   ],
   "source": [
    "class community_function:\n",
    "  def __init__(self, fileNames):\n",
    "    self.fileNames = fileNames\n",
    "    self.G = []\n",
    "    for fileName in self.fileNames:\n",
    "      g = self.getNeighbourMatrixFromFile(fileName)\n",
    "      self.G.append(g)\n",
    "    self.dim = len(list(self.G[0].nodes))\n",
    "    self.layers = len(fileNames)\n",
    "      \n",
    "  def fitness(self, x):\n",
    "    partition = {}\n",
    "    for val, ind in zip(np.nditer(x), np.ndindex(x.shape[0])):\n",
    "      partition[ind[0]] = int(val)\n",
    "      \n",
    "    modularity = 0\n",
    "    for g in self.G:\n",
    "      modularity += comm.modularity(partition, g)\n",
    "    return [-modularity]\n",
    "\n",
    "  def get_bounds(self):\n",
    "    return ([1] * self.dim, [self.dim] * self.dim)\n",
    "  \n",
    "  def get_nix(self):\n",
    "    # Number of integer dimensions (all of them)\n",
    "    print(self.dim)\n",
    "    return self.dim\n",
    "  \n",
    "  def get_name(self):\n",
    "    ret = \"Best partition for multiplex graph\\n\"\n",
    "    for fileName in self.fileNames:\n",
    "      ret +=\"File: \" + fileName + \"\\n\"\n",
    "    ret += \"Dimension: \" + str(self.dim)\n",
    "    ret += \"\\n\\n\"\n",
    "    return ret\n",
    "  \n",
    "  def getNeighbourMatrixFromFile(self, fileName):\n",
    "    A = pd.read_csv(fileName, sep=\" \", header=None)\n",
    "    G = nx.from_numpy_matrix(np.array(A))\n",
    "    return G\n",
    "  \n",
    "  def drawPartition(self, graphIndex=0, partition=None, fun=nx.spring_layout):\n",
    "    if partition is None:\n",
    "      raise ValueError(\"No partition passed as parameter!!\")\n",
    "    if type(partition) is list:\n",
    "      partition = {key:val for key, val in enumerate(partition)}\n",
    "    if type(partition) is not dict:\n",
    "      raise ValueError(\"Invalid partititon as input: \" + str(type(partition)))\n",
    "\n",
    "    plt.clf()\n",
    "      \n",
    "    colors = self.getColors()\n",
    "    G = self.G[graphIndex]\n",
    "    pos = fun(G)\n",
    "    count = 0\n",
    "    \n",
    "    for com in set(partition.values()):\n",
    "      toDraw = True\n",
    "      list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "      labels = dict()\n",
    "      for node in list_nodes:\n",
    "        labels[node] = node+1\n",
    "      node_colors = np.zeros((len(list_nodes), 3))\n",
    "      if len(list_nodes) <= 3:\n",
    "        node_colors[:] = [1, 1, 1]\n",
    "        if fun != nx.circular_layout:\n",
    "          toDraw = False\n",
    "      else:\n",
    "        try:\n",
    "          node_colors[:] = colors[count]\n",
    "          count += 1\n",
    "        except:\n",
    "          node_colors[:] = [1, 1, 1]\n",
    "      if toDraw:\n",
    "        nx.draw_networkx_nodes(G, pos, list_nodes, node_size = 20, node_color=node_colors)\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size = 3)\n",
    "    nx.draw_networkx_edges(G, pos, width=0.2)\n",
    "    \n",
    "    outfile=self.fileNames[graphIndex] + \"_moeaGenerated.png\"\n",
    "    plt.savefig(outfile, dpi=300)\n",
    "    \n",
    "  def getColors(self):\n",
    "    return np.array([\n",
    "            [230, 25, 75],\n",
    "            [60, 180, 75],\n",
    "            [255, 225, 25],\n",
    "            [0, 130, 200],\n",
    "            [245, 130, 48],\n",
    "            [145, 30, 180],\n",
    "            [70, 240, 240],\n",
    "            [240, 50, 230],\n",
    "            [210, 245, 60],\n",
    "            [250, 190, 190],\n",
    "            [0, 128, 128],\n",
    "            [230, 190, 255],\n",
    "            [170, 110, 40],\n",
    "            [255, 250, 200],\n",
    "            [128, 0, 0],\n",
    "            [170, 255, 195],\n",
    "            [128, 128, 0],\n",
    "            [255, 215, 180],\n",
    "            [0, 0, 128],\n",
    "            [128, 128, 128],\n",
    "            [0, 0, 0]\n",
    "          ])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 281 µs\n"
     ]
    }
   ],
   "source": [
    "# problem = pg.problem(community_function(_3layer_mu0_1_multi))\n",
    "# algo = pg.algorithm(pg.sga(gen=20))\n",
    "# population = pg.population(problem, 3)\n",
    "# algo.set_verbosity(2)\n",
    "# population = algo.evolve(population)\n",
    "# # # population.champion_x\n",
    "# # # population.champion_f\n",
    "# # champion_x = population.champion_x\n",
    "# # df = pd.DataFrame()\n",
    "# # df[0] = list(range(0, len(champion_x)))\n",
    "# # df[1] = champion_x.astype(int)\n",
    "# # df.to_csv('test6_single.txt', sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "4layermu0_all_community_run2.txt\n",
      "[0.89945393]\n",
      "(15, 0, 34)\n",
      "\n",
      "128\n",
      "4layermu0.1_all_community_run2.txt\n",
      "[1.16797935]\n",
      "(15, 20, 13)\n",
      "\n",
      "128\n",
      "4layermu0.2_all_community_run2.txt\n",
      "[2.17338463]\n",
      "(15, 38, 56)\n",
      "\n",
      "128\n",
      "4layermu0.3_all_community_run2.txt\n",
      "[2.13546734]\n",
      "(15, 57, 56)\n",
      "\n",
      "128\n",
      "4layermu0.4_all_community_run2.txt\n",
      "[2.59109639]\n",
      "(16, 16, 59)\n",
      "\n",
      "128\n",
      "4layermu0.5_all_community_run2.txt\n",
      "[2.57176881]\n",
      "(16, 34, 13)\n",
      "\n",
      "time: 1h 53min 10s\n"
     ]
    }
   ],
   "source": [
    "for data in testData:\n",
    "  outFileName = ''.join(data[0].split('_')[0:2]).split('/')[1] + '_all_community_run3.txt'\n",
    "  os.write(1, outFileName.encode())\n",
    "\n",
    "  problem = pg.problem(community_function(data))\n",
    "  algo = pg.algorithm(pg.sga(gen=10000))\n",
    "  algo.set_verbosity(100)\n",
    "  archi = pg.archipelago(n=3, algo=algo, prob=problem, pop_size=50)\n",
    "  archi.evolve()\n",
    "  archi.wait()\n",
    "  \n",
    "  champions = archi.get_champions_f()\n",
    "  archi.get_champions_x()\n",
    "  globalMin = min(champions)\n",
    "  championIndex = champions.index(globalMin)\n",
    "  champion_x = archi.get_champions_x()[championIndex]\n",
    "    \n",
    "  df = pd.DataFrame()\n",
    "  df[0] = list(range(0, len(champion_x)))\n",
    "  df[1] = champion_x.astype(int)\n",
    "  \n",
    "  df.to_csv(outFileName, sep=' ', header=False, index=False)\n",
    "  \n",
    "  print(outFileName)\n",
    "  print(globalMin)\n",
    "  \n",
    "  now = datetime.datetime.now()\n",
    "  time_now = (now.hour, now.minute, now.second)\n",
    "  print(time_now)\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 372 µs\n"
     ]
    }
   ],
   "source": [
    "# # problem = pg.problem(community_function(aarhus))\n",
    "# # problem = pg.problem(community_function(euair))\n",
    "# # problem = pg.problem(community_function(krackhardt))\n",
    "# # problem = pg.problem(community_function(padgett))\n",
    "# # problem = pg.problem(community_function(_4layers))\n",
    "# problem = pg.problem(community_function(_3layers))\n",
    "\n",
    "# # Single objective functions\n",
    "# algo = pg.algorithm(pg.bee_colony(gen = 500, limit = 20))\n",
    "# population = pg.population(problem, 10)\n",
    "# population = algo.evolve(population)\n",
    "# print(population.champion_f)\n",
    "# print(population.champion_x)\n",
    "\n",
    "# algo = pg.algorithm(pg.de(gen = 500))\n",
    "# population = pg.population(problem, 10)\n",
    "# population = algo.evolve(population)\n",
    "# print(population.champion_f)\n",
    "# print(population.champion_x)\n",
    "\n",
    "# algo = pg.algorithm(pg.sea(500))\n",
    "# population = pg.population(problem, 10)\n",
    "# population = algo.evolve(population)\n",
    "# print(population.champion_f)\n",
    "# print(population.champion_x)\n",
    "\n",
    "# algo = pg.algorithm(pg.sga(gen = 500))\n",
    "# population = pg.population(problem, 10)\n",
    "# population = algo.evolve(population)\n",
    "# print(population.champion_f)\n",
    "# print(population.champion_x)\n",
    "\n",
    "# algo = pg.algorithm(pg.sade(gen = 500))\n",
    "# population = pg.population(problem, 10)\n",
    "# population = algo.evolve(population)\n",
    "# print(population.champion_f)\n",
    "# print(population.champion_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 269 µs\n"
     ]
    }
   ],
   "source": [
    "# # MultiObjective\n",
    "# # Any way to use it?\n",
    "\n",
    "# algo = pg.algorithm(pg.nsga2(gen=100))\n",
    "# population = pg.population(problem, 10)\n",
    "# population = algo.evolve(population)\n",
    "# print(population.champion_f)\n",
    "# print(population.champion_x)\n",
    "\n",
    "# algo = pg.algorithm(pg.moead(gen=500))\n",
    "# population = pg.population(problem, 10)\n",
    "# population = algo.evolve(population)\n",
    "# print(population.champion_f)\n",
    "# print(population.champion_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
